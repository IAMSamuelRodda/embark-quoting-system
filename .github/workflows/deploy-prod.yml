name: Deploy to Production

on:
  push:
    tags:
      - 'v*.*.*'  # Trigger on version tags like v1.0.0, v1.2.3
  workflow_dispatch:  # Allow manual triggers with approval

env:
  AWS_REGION: ap-southeast-2  # Sydney region (matches deployed infrastructure)
  ECR_REPOSITORY: embark-quoting-backend
  ECS_CLUSTER: embark-quoting-cluster
  ECS_SERVICE: embark-backend-service
  ECS_TASK_DEFINITION: embark-quoting-backend
  CONTAINER_NAME: embark-backend

jobs:
  pre-deployment-checks:
    name: Pre-Deployment Validation
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate tag format
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}
          echo "Deploying tag: $TAG_NAME"

          if [[ ! $TAG_NAME =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            echo "‚ùå Invalid tag format. Expected: v1.0.0"
            exit 1
          fi

          echo "‚úÖ Tag format valid"

      - name: Check if tag exists in ECR
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}
          echo "Checking if image exists for tag: $TAG_NAME"

          # TODO: Implement check that image was built and pushed from main branch
          # This ensures we only deploy images that passed all CI checks

          echo "‚ö†Ô∏è  Image validation not yet implemented"

  deploy-backend:
    name: Deploy Backend to Production ECS
    runs-on: ubuntu-latest
    needs: [pre-deployment-checks]
    environment:
      # Note: Create 'production' environment in GitHub repo settings to enable manual approvals
      # Settings > Environments > New environment > Name: production
      name: production
      url: https://api.embark-quoting.com  # TODO: Update to your production API URL

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_PROD_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Get image URI for tag
        id: get-image
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}
          # Use 'latest' tag since Build workflow tags with latest/main/main-{shortSHA}
          # Production deployments use same 'latest' image that passed staging tests
          IMAGE_TAG="latest"
          IMAGE_URI="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${IMAGE_TAG}"
          echo "image-uri=${IMAGE_URI}" >> $GITHUB_OUTPUT
          echo "Deploying tagged release ${TAG_NAME} using image: ${IMAGE_URI}"

      - name: Download current task definition
        run: |
          aws ecs describe-task-definition \
            --task-definition ${{ env.ECS_TASK_DEFINITION }} \
            --query taskDefinition \
            > task-definition.json

          # Save current task definition for potential rollback
          CURRENT_REVISION=$(aws ecs describe-services \
            --cluster ${{ env.ECS_CLUSTER }} \
            --services ${{ env.ECS_SERVICE }} \
            --query 'services[0].taskDefinition' \
            --output text)

          echo "current-revision=${CURRENT_REVISION}" >> $GITHUB_ENV
          echo "Current revision: ${CURRENT_REVISION}"

      - name: Update task definition with new image
        id: task-def
        uses: aws-actions/amazon-ecs-render-task-definition@v1
        with:
          task-definition: task-definition.json
          container-name: ${{ env.CONTAINER_NAME }}
          image: ${{ steps.get-image.outputs.image-uri }}

      - name: Deploy to Amazon ECS (Blue-Green)
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          task-definition: ${{ steps.task-def.outputs.task-definition }}
          service: ${{ env.ECS_SERVICE }}
          cluster: ${{ env.ECS_CLUSTER }}
          wait-for-service-stability: true
          wait-for-minutes: 15
          # ECS will perform a rolling deployment with health checks

      - name: Verify deployment health
        run: |
          echo "Waiting for new tasks to become healthy..."
          sleep 60

          # Get ALB URL
          ALB_URL="${{ secrets.PROD_API_URL }}"

          # Health check
          echo "Running health check on ${ALB_URL}/health"
          for i in {1..5}; do
            response=$(curl -s -o /dev/null -w "%{http_code}" ${ALB_URL}/health)

            if [ "$response" = "200" ]; then
              echo "‚úÖ Health check passed (attempt $i/5)"
              break
            else
              echo "‚ö†Ô∏è  Health check failed (attempt $i/5): $response"
              if [ $i -eq 5 ]; then
                echo "‚ùå Deployment verification failed after 5 attempts"
                echo "ROLLBACK_REQUIRED=true" >> $GITHUB_ENV
                exit 1
              fi
              sleep 10
            fi
          done

      - name: Basic smoke test
        run: |
          ALB_URL="${{ secrets.PROD_API_URL }}"

          echo "Running basic smoke test..."

          # Simple health endpoint check (no grep, just HTTP 200)
          if curl -f -s -o /dev/null -w "%{http_code}" ${ALB_URL}/health | grep -q "200"; then
            echo "‚úÖ Health endpoint responding"
          else
            echo "‚ùå Health endpoint failed"
            echo "ROLLBACK_REQUIRED=true" >> $GITHUB_ENV
            exit 1
          fi

      - name: Rollback on failure
        if: failure() && env.ROLLBACK_REQUIRED == 'true'
        run: |
          echo "üîÑ Rolling back to previous task definition: ${{ env.current-revision }}"

          aws ecs update-service \
            --cluster ${{ env.ECS_CLUSTER }} \
            --service ${{ env.ECS_SERVICE }} \
            --task-definition ${{ env.current-revision }} \
            --force-new-deployment

          echo "‚è≥ Waiting for rollback to complete..."
          aws ecs wait services-stable \
            --cluster ${{ env.ECS_CLUSTER }} \
            --services ${{ env.ECS_SERVICE }}

          echo "‚úÖ Rollback complete"
          exit 1

  deploy-frontend:
    name: Deploy Frontend to Production S3/CloudFront
    runs-on: ubuntu-latest
    needs: [deploy-backend]  # Wait for backend to be healthy first
    environment:
      # Note: Uses same 'production' environment as deploy-backend job
      name: production
      url: https://embark-quoting.com  # TODO: Update to your production frontend URL

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: ./frontend
        run: npm ci

      - name: Build frontend for production
        working-directory: ./frontend
        run: npm run build
        env:
          NODE_ENV: production
          VITE_API_URL: ${{ secrets.PROD_API_URL }}
          VITE_COGNITO_USER_POOL_ID: ${{ secrets.PROD_COGNITO_USER_POOL_ID }}
          VITE_COGNITO_CLIENT_ID: ${{ secrets.PROD_COGNITO_CLIENT_ID }}
          VITE_COGNITO_REGION: ${{ env.AWS_REGION }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_PROD_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy to S3
        run: |
          # Sync all files except index.html and service-worker.js with long cache
          aws s3 sync ./frontend/dist/ s3://${{ secrets.PROD_S3_BUCKET }}/ \
            --delete \
            --cache-control "public, max-age=31536000, immutable" \
            --exclude "index.html" \
            --exclude "service-worker.js"

          # Upload index.html and service-worker.js with no cache
          aws s3 cp ./frontend/dist/index.html s3://${{ secrets.PROD_S3_BUCKET }}/index.html \
            --cache-control "public, max-age=0, must-revalidate" \
            --metadata-directive REPLACE

          if [ -f ./frontend/dist/service-worker.js ]; then
            aws s3 cp ./frontend/dist/service-worker.js s3://${{ secrets.PROD_S3_BUCKET }}/service-worker.js \
              --cache-control "public, max-age=0, must-revalidate" \
              --metadata-directive REPLACE
          fi

      - name: Create CloudFront invalidation
        id: invalidation
        run: |
          INVALIDATION_ID=$(aws cloudfront create-invalidation \
            --distribution-id ${{ secrets.PROD_CLOUDFRONT_ID }} \
            --paths "/*" \
            --query 'Invalidation.Id' \
            --output text)

          echo "invalidation-id=${INVALIDATION_ID}" >> $GITHUB_OUTPUT
          echo "Created CloudFront invalidation: ${INVALIDATION_ID}"

      - name: Wait for CloudFront invalidation
        run: |
          echo "Waiting for CloudFront invalidation to complete..."
          aws cloudfront wait invalidation-completed \
            --distribution-id ${{ secrets.PROD_CLOUDFRONT_ID }} \
            --id ${{ steps.invalidation.outputs.invalidation-id }}

          echo "‚úÖ CloudFront cache invalidated"

      - name: Verify frontend deployment
        run: |
          sleep 30

          response=$(curl -s -o /dev/null -w "%{http_code}" ${{ secrets.PROD_FRONTEND_URL }})

          if [ "$response" = "200" ]; then
            echo "‚úÖ Frontend deployment successful!"
          else
            echo "‚ùå Frontend deployment verification failed! Response: $response"
            exit 1
          fi

  # Amazon-Style Testing: Minimal smoke tests on production
  # Comprehensive E2E tests already ran on staging (see deploy-staging.yml)
  # This workflow REQUIRES smoke tests to pass before deployment is considered complete
  deployment-verification:
    name: Smoke Tests (Production)
    needs: [deploy-backend, deploy-frontend]
    uses: ./.github/workflows/e2e-tests.yml
    with:
      environment: production
      # Run ONLY authentication validation tests (fast smoke test)
      # Full test suite already validated on staging
      test-pattern: 'e2e/auth.spec.ts'
    secrets:
      # Pass URLs through secrets section (cannot use secrets.* in with: section)
      base-url-secret: ${{ secrets.PROD_FRONTEND_URL }}
      api-url-secret: ${{ secrets.PROD_API_URL }}
      test-user-email: ${{ secrets.E2E_PROD_USER_EMAIL }}
      test-user-password: ${{ secrets.E2E_PROD_USER_PASSWORD }}

  monitor-deployment:
    name: Monitor Production Metrics
    runs-on: ubuntu-latest
    needs: [deployment-verification]
    timeout-minutes: 10

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_PROD_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Monitor CloudWatch metrics for degradation
        run: |
          echo "üìä Monitoring production metrics for 5 minutes..."
          echo "Watching: HTTP 5xx errors, target health, response times"
          echo ""

          # Get ALB ARN suffix for CloudWatch metrics
          # TODO: Store ALB_ARN_SUFFIX in GitHub Secrets during infrastructure setup
          ALB_ARN_SUFFIX="${{ secrets.PROD_ALB_ARN_SUFFIX }}"

          if [ -z "$ALB_ARN_SUFFIX" ]; then
            echo "‚ö†Ô∏è  PROD_ALB_ARN_SUFFIX not configured - skipping metrics monitoring"
            echo "To enable automatic rollback based on metrics, add ALB ARN suffix to secrets"
            exit 0
          fi

          ROLLBACK_TRIGGERED=false

          for i in {1..10}; do
            echo "Check $i/10 (monitoring for 30s intervals)..."

            # Get HTTP 5xx error count from last 60 seconds
            ERROR_COUNT=$(aws cloudwatch get-metric-statistics \
              --namespace AWS/ApplicationELB \
              --metric-name HTTPCode_Target_5XX_Count \
              --dimensions Name=LoadBalancer,Value=${ALB_ARN_SUFFIX} \
              --start-time $(date -u -d '60 seconds ago' +%Y-%m-%dT%H:%M:%S) \
              --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
              --period 60 \
              --statistics Sum \
              --query 'Datapoints[0].Sum' \
              --output text 2>/dev/null || echo "0")

            # Get unhealthy target count
            UNHEALTHY_COUNT=$(aws cloudwatch get-metric-statistics \
              --namespace AWS/ApplicationELB \
              --metric-name UnHealthyHostCount \
              --dimensions Name=LoadBalancer,Value=${ALB_ARN_SUFFIX} \
              --start-time $(date -u -d '60 seconds ago' +%Y-%m-%dT%H:%M:%S) \
              --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
              --period 60 \
              --statistics Maximum \
              --query 'Datapoints[0].Maximum' \
              --output text 2>/dev/null || echo "0")

            # Handle "None" or empty responses
            [ "$ERROR_COUNT" = "None" ] || [ -z "$ERROR_COUNT" ] && ERROR_COUNT=0
            [ "$UNHEALTHY_COUNT" = "None" ] || [ -z "$UNHEALTHY_COUNT" ] && UNHEALTHY_COUNT=0

            echo "  - 5xx errors: $ERROR_COUNT"
            echo "  - Unhealthy targets: $UNHEALTHY_COUNT"

            # Trigger rollback if error rate is too high
            if [ "$ERROR_COUNT" != "0" ] && [ "$ERROR_COUNT" -gt 10 ]; then
              echo "‚ùå High error rate detected: $ERROR_COUNT errors in 60s (threshold: 10)"
              ROLLBACK_TRIGGERED=true
              break
            fi

            # Trigger rollback if targets are unhealthy
            if [ "$UNHEALTHY_COUNT" != "0" ] && [ "$UNHEALTHY_COUNT" -gt 0 ]; then
              echo "‚ùå Unhealthy targets detected: $UNHEALTHY_COUNT"
              ROLLBACK_TRIGGERED=true
              break
            fi

            echo "  ‚úÖ Metrics healthy"
            sleep 30
          done

          if [ "$ROLLBACK_TRIGGERED" = true ]; then
            echo ""
            echo "‚ùå Production metrics degraded - automatic rollback recommended"
            echo "Manual rollback: aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --task-definition <previous-revision>"
            exit 1
          fi

          echo ""
          echo "‚úÖ Production metrics stable for 5 minutes"
          echo "Deployment monitoring complete"

  create-release:
    name: Create GitHub Release
    runs-on: ubuntu-latest
    needs: [deploy-backend, deploy-frontend]

    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for changelog

      - name: Generate changelog
        id: changelog
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}
          PREV_TAG=$(git describe --tags --abbrev=0 $TAG_NAME^ 2>/dev/null || echo "")

          if [ -z "$PREV_TAG" ]; then
            echo "First release - no previous tag"
            CHANGELOG="Initial release of Embark Quoting System"
          else
            echo "Generating changelog from $PREV_TAG to $TAG_NAME"
            CHANGELOG=$(git log --pretty=format:"- %s (%h)" $PREV_TAG..$TAG_NAME)
          fi

          # Save changelog to file
          echo "$CHANGELOG" > CHANGELOG.txt
          echo "Generated changelog"

      - name: Create GitHub Release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}

          gh release create "$TAG_NAME" \
            --title "Release $TAG_NAME" \
            --notes-file CHANGELOG.txt \
            --verify-tag

          echo "‚úÖ GitHub release created: $TAG_NAME"

  notify-deployment:
    name: Notify Deployment Success
    runs-on: ubuntu-latest
    needs: [create-release, monitor-deployment]
    if: success()

    steps:
      - name: Deployment notification
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}
          echo "üéâ Deployment successful!"
          echo "Version: $TAG_NAME"
          echo "Frontend: ${{ secrets.PROD_FRONTEND_URL }}"
          echo "API: ${{ secrets.PROD_API_URL }}"
        # TODO: Add Slack/email notification integration

  notify-failure:
    name: Notify Deployment Failure
    runs-on: ubuntu-latest
    needs: [deploy-backend, deploy-frontend, deployment-verification, monitor-deployment]
    if: failure()

    steps:
      - name: Failure notification
        run: |
          TAG_NAME=${GITHUB_REF#refs/tags/}
          echo "‚ùå Production deployment failed for $TAG_NAME"
          echo "Check workflow logs and rollback if necessary"
        # TODO: Add urgent Slack/email notification for failures
